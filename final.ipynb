{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z_rshfDztvL8",
   "metadata": {
    "id": "Z_rshfDztvL8"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yc-wYr2xjQD0",
   "metadata": {
    "id": "yc-wYr2xjQD0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AsCsdt426xuH",
   "metadata": {
    "id": "AsCsdt426xuH"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "!pip install readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-large",
   "metadata": {
    "id": "angry-large"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import gensim.downloader\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OCGK5ZxK9hPC",
   "metadata": {
    "id": "OCGK5ZxK9hPC"
   },
   "outputs": [],
   "source": [
    "conll2000_dataset = pd.read_csv('data2.csv', usecols=('content', 'bias'), nrows=5000)\n",
    "#conll2000_dataset['ID'] = conll2000_dataset['ID'].apply(lambda x: ast.literal_eval(x))\n",
    "#conll2000_dataset['content_original'] = conll2000_dataset['content_original'].apply(lambda x: ast.literal_eval(x))\n",
    "#conll2000_dataset['content_original'] = conll2000_dataset['content_original'].apply(lambda x: ast.literal_eval(x))\n",
    "#conll2000_dataset['bias_text'] = conll2000_dataset['bias_text'].apply(lambda x: ast.literal_eval(x))\n",
    "#conll2000_dataset['bias'] = conll2000_dataset['bias'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "conll2000_dataset[\"content\"] = [nltk.word_tokenize(row) for row in conll2000_dataset[\"content\"]]\n",
    "\n",
    "display(conll2000_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BxAPZwXCpiXE",
   "metadata": {
    "id": "BxAPZwXCpiXE"
   },
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = DistilBertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZA4YeQwvqd8b",
   "metadata": {
    "id": "ZA4YeQwvqd8b"
   },
   "outputs": [],
   "source": [
    "# Get the tokens for the first data entry\n",
    "tokens = conll2000_dataset.iloc[0]['content']\n",
    "print('Original tokens:')\n",
    "print(tokens)\n",
    "\n",
    "# Convert these tokens into DistilBERT's token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print('Token IDs:')\n",
    "print(token_ids)\n",
    "\n",
    "# Convert the token IDs back into regular tokens\n",
    "tokens_backtranslated = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print('Backtranslated tokens:')\n",
    "print(tokens_backtranslated)\n",
    "# Notice that it automatically does UNK replacement for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D1uggSbYt9vp",
   "metadata": {
    "id": "D1uggSbYt9vp"
   },
   "outputs": [],
   "source": [
    "from fine_tune import POSTagDataset\n",
    "\n",
    "dataset = POSTagDataset(conll2000_dataset, tokenizer)\n",
    "(train, val, test) = torch.utils.data.random_split(dataset, [0.8, 0.1, 0.1])\n",
    "\n",
    "print(train[0]) # verify that this has the correct structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Si17k5QQHFsa",
   "metadata": {
    "id": "Si17k5QQHFsa"
   },
   "outputs": [],
   "source": [
    "from fine_tune import basic_collate_fn\n",
    "\n",
    "# grab a test minibatch\n",
    "test_minibatch = [train[0], train[1]]\n",
    "batch_in, batch_out = basic_collate_fn(test_minibatch)\n",
    "print(batch_in['input_ids'].size())\n",
    "print(batch_in[\"attention_mask\"].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5g0I2usFrGD",
   "metadata": {
    "id": "b5g0I2usFrGD"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train, batch_size=64, collate_fn=basic_collate_fn, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=64, collate_fn=basic_collate_fn, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=64, collate_fn=basic_collate_fn, shuffle=False)\n",
    "\n",
    "batch_in, pos_ids = next(iter(train_loader))\n",
    "print(batch_in['input_ids'].size())\n",
    "print(batch_in['attention_mask'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dp54Ov4AEvJB",
   "metadata": {
    "id": "dp54Ov4AEvJB"
   },
   "outputs": [],
   "source": [
    "from fine_tune import DistilBertForTokenClassification\n",
    "\n",
    "hidden_dim = 768 # this is fixed for BERT models\n",
    "#num_pos_tags =  len(dataset.pos_tags.keys())\n",
    "num_pos_tags = 3\n",
    "model = DistilBertForTokenClassification(bert_model, hidden_dim, num_pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d90ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(**batch_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cvAEuKr4F4QX",
   "metadata": {
    "id": "cvAEuKr4F4QX"
   },
   "outputs": [],
   "source": [
    "output = model(**batch_in)\n",
    "print(output.size())\n",
    "print(num_pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahvM-7NEuNBO",
   "metadata": {
    "id": "ahvM-7NEuNBO"
   },
   "outputs": [],
   "source": [
    "from fine_tune import get_loss_fn, calculate_loss\n",
    "\n",
    "loss_fn = get_loss_fn()\n",
    "print(f\"Loss function: {loss_fn}\")\n",
    "\n",
    "# Test loss of randomly generated labels and scores\n",
    "labels = torch.randint(0, num_pos_tags, (10,))\n",
    "print(labels)\n",
    "scores = torch.rand((10, num_pos_tags))\n",
    "loss = calculate_loss(scores, labels, loss_fn)\n",
    "print(f\"Loss: {loss}\")\n",
    "\n",
    "# Test loss of padded labels\n",
    "labels = torch.zeros((10,), dtype=torch.long) # make the POS tags all padding tokens\n",
    "labels[0] = 1 # except one; set this one to an arbitrary value\n",
    "#labels[1] = 10\n",
    "print(labels)\n",
    "loss = calculate_loss(scores, labels, loss_fn)\n",
    "print(f\"Loss of padded inputs: {loss}\")\n",
    "scores[1:, :] = torch.rand((9, num_pos_tags)) # change the values of the rest of the tokens\n",
    "print(labels)\n",
    "loss = calculate_loss(scores, labels, loss_fn)\n",
    "print(f\"Loss of modified padded: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba1210",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dq0RN0GI6Mp2",
   "metadata": {
    "id": "dq0RN0GI6Mp2"
   },
   "outputs": [],
   "source": [
    "from fine_tune import get_optimizer, train_model\n",
    "\n",
    "# Run this lines to reload the model\n",
    "bert_model = DistilBertModel.from_pretrained(model_name)\n",
    "model = DistilBertForTokenClassification(bert_model, hidden_dim, num_pos_tags)\n",
    "\n",
    "# Run some test optimization\n",
    "model.to(device)\n",
    "optim = get_optimizer(model, lr=5e-5, weight_decay=0)\n",
    "best_model, stats = train_model(model, val_loader, val_loader, optim,\n",
    "                                num_epoch=25, collect_cycle=5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DW_lJgx2C4Bz",
   "metadata": {
    "id": "DW_lJgx2C4Bz"
   },
   "outputs": [],
   "source": [
    "from fine_tune import plot_loss\n",
    "plot_loss(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LLS10L3o8Jx_",
   "metadata": {
    "id": "LLS10L3o8Jx_"
   },
   "outputs": [],
   "source": [
    "from fine_tune import get_optimizer, train_model\n",
    "\n",
    "# Run this lines to reload the model\n",
    "bert_model = DistilBertModel.from_pretrained(model_name)\n",
    "model = DistilBertForTokenClassification(bert_model, 768, num_pos_tags)\n",
    "\n",
    "# Run the full optimization\n",
    "model.to(device)\n",
    "optim = get_optimizer(model, lr=0.0001, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YG5_Zh9Dwch4",
   "metadata": {
    "id": "YG5_Zh9Dwch4"
   },
   "outputs": [],
   "source": [
    "from fine_tune import plot_loss\n",
    "plot_loss(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oW--ke6ZGL-x",
   "metadata": {
    "id": "oW--ke6ZGL-x"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "from fine_tune import get_hyper_parameters, plot_loss\n",
    "\n",
    "def search_param_utterance():\n",
    "    \"\"\"Experiemnt on different hyper parameters.\"\"\"\n",
    "    learning_rate, weight_decay = get_hyper_parameters()\n",
    "    print(\"learning rate from: {}\\nweight_decay from: {}\".format(\n",
    "        learning_rate, weight_decay\n",
    "    ))\n",
    "    best_model, best_stats = None, None\n",
    "    best_accuracy, best_lr, best_wd, best_hd = 0, 0, 0, 0\n",
    "    for lr, wd in tqdm(itertools.product(learning_rate, weight_decay),\n",
    "                           total=len(learning_rate) * len(weight_decay)):\n",
    "        ############################## START OF YOUR CODE ##############################\n",
    "        \n",
    "        bert = DistilBertModel.from_pretrained(model_name)\n",
    "        model = DistilBertForTokenClassification(bert, 768, num_pos_tags).to(device)\n",
    "        optim = get_optimizer(model, lr=lr, weight_decay=wd)\n",
    "        model, stats = train_model(model, train_loader, val_loader, optim, num_epoch=24, collect_cycle=20, device=device)\n",
    "        if stats[\"accuracy\"] > best_accuracy:\n",
    "            best_model = model\n",
    "            best_stats = stats\n",
    "            best_accuracy = stats[\"accuracy\"]\n",
    "            best_lr = lr\n",
    "            best_wd = wd\n",
    "\n",
    "        ############################### END OF YOUR CODE ###############################\n",
    "    print(\"\\n\\nBest learning rate: {}, best weight_decay: {}\".format(best_lr, best_wd))\n",
    "    print(\"Accuracy: {:.4f}\".format(best_accuracy))\n",
    "    plot_loss(best_stats)\n",
    "    return best_model\n",
    "best_model = search_param_utterance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34M9Es0-YOpV",
   "metadata": {
    "id": "34M9Es0-YOpV"
   },
   "outputs": [],
   "source": [
    "from fine_tune import get_validation_performance\n",
    "\n",
    "get_validation_performance(best_model, get_loss_fn(), test_loader, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G41koFqvPOPB",
   "metadata": {
    "id": "G41koFqvPOPB"
   },
   "outputs": [],
   "source": [
    "from fine_tune import make_prediction\n",
    "\n",
    "y_true, y_pred, errors = make_prediction(best_model, test_loader, 'cuda')\n",
    "print(test[0]['tokens'], y_true[1], y_pred[1])\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fc7457-6074-40a9-8b83-753869ac74b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = [\"left\", \"center\", \"right\"]\n",
    "cm = confusion_matrix(y_true, y_pred, normalize=\"true\")\n",
    "_, ax = plt.subplots(figsize=(3, 3))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "plt.title(\"Normalized confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09bac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2000_dataset_test = pd.read_csv('conll2000_test_unknown.csv', usecols=('tokens', 'pos_tags'))\n",
    "conll2000_dataset_test['tokens'] = conll2000_dataset_test['tokens'].apply(lambda x: ast.literal_eval(x))\n",
    "conll2000_dataset_test['pos_tags'] = conll2000_dataset_test['pos_tags'].apply(lambda x: ast.literal_eval(x))\n",
    "display(conll2000_dataset_test)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
